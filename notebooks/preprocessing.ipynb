{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff304fae",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "774a69c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T14:43:32.649219Z",
     "start_time": "2022-02-08T14:43:30.159544Z"
    }
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import csv\n",
    "import pickle\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e834accd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T14:43:33.283931Z",
     "start_time": "2022-02-08T14:43:33.277415Z"
    }
   },
   "outputs": [],
   "source": [
    "## Utilities variables\n",
    "\n",
    "#Sample mode \n",
    "SAMPLE_MODE = None\n",
    "\n",
    "#Max len of essay\n",
    "SEQ_LEN = 1024\n",
    "\n",
    "#path\n",
    "PATH_RAW_DATA='/Users/arthurcollard/code/arthurcol/feedback_prize/raw_data/'\n",
    "\n",
    "VERSION = 3\n",
    "NAME_OUTPUT_FILE = f'preprocessed_v{VERSION}.csv'\n",
    "NAME_TEST_FILE = f'test_preprocessed_v{VERSION}.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17de51a",
   "metadata": {},
   "source": [
    "# Data loading and preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91aae15f",
   "metadata": {},
   "source": [
    "## Loading training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76f1bd5e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T14:43:34.814320Z",
     "start_time": "2022-02-08T14:43:34.053319Z"
    }
   },
   "outputs": [],
   "source": [
    "#load data from csv file \n",
    "df = pd.read_csv(PATH_RAW_DATA+'train.csv',nrows=SAMPLE_MODE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc7bdb6",
   "metadata": {},
   "source": [
    "## Preparation of the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4867ca17",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T14:43:35.401140Z",
     "start_time": "2022-02-08T14:43:35.397688Z"
    }
   },
   "outputs": [],
   "source": [
    "## Def a function for labelling discourses per word\n",
    "\n",
    "def labelizer(label,len_,flag):\n",
    "    \"\"\"Repeat the label according to the length of the sentence. Makes use of B/I notation according to the position of the word within the sentence and the sentence within the essay.\n",
    "\n",
    "    Args:\n",
    "        label (str): NER label of the sentence.\n",
    "        len_ (int): Length of the sentence (n° of words).\n",
    "        flag (int): 1 if the sentence follows a sentence with the same label. 0 otherwise.\n",
    "\n",
    "    Returns:\n",
    "        str: Returns a string of length (n° of words) len_ with B/I-label repeated len_ times.\n",
    "    \"\"\"\n",
    "    if flag==0:\n",
    "        label_first = f'B-{label} '\n",
    "    else:\n",
    "        label_first = f'I-{label} '\n",
    "        \n",
    "    return (label_first + f'I-{label} '*(len_-1)).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b65d4e94",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T14:43:36.153643Z",
     "start_time": "2022-02-08T14:43:35.728659Z"
    }
   },
   "outputs": [],
   "source": [
    "## Creating features for labeling needs : \n",
    "\n",
    "    #Flag if the discourse is the same as the previous one\n",
    "df['previous_discourse_flag']=np.where(df['discourse_type'].shift(1)==df['discourse_type'],1,0)\n",
    "\n",
    "    #Get length of predictionstring\n",
    "df['predictionstring_len'] = df['predictionstring'].apply(lambda txt:len(txt.split()))\n",
    "\n",
    "    # Remove spaces in labels\n",
    "df['discourse_type']=df['discourse_type'].str.replace('Concluding Statement','Concluding_Statement')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73c807ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T14:43:56.325010Z",
     "start_time": "2022-02-08T14:43:36.182014Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>discourse_id</th>\n",
       "      <th>discourse_start</th>\n",
       "      <th>discourse_end</th>\n",
       "      <th>discourse_text</th>\n",
       "      <th>discourse_type</th>\n",
       "      <th>discourse_type_num</th>\n",
       "      <th>predictionstring</th>\n",
       "      <th>previous_discourse_flag</th>\n",
       "      <th>predictionstring_len</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>1.622628e+12</td>\n",
       "      <td>8.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>Modern humans today are always on their phone....</td>\n",
       "      <td>Lead</td>\n",
       "      <td>Lead 1</td>\n",
       "      <td>1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 1...</td>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>B-Lead I-Lead I-Lead I-Lead I-Lead I-Lead I-Le...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id  discourse_id  discourse_start  discourse_end  \\\n",
       "0  423A1CA112E2  1.622628e+12              8.0          229.0   \n",
       "\n",
       "                                      discourse_text discourse_type  \\\n",
       "0  Modern humans today are always on their phone....           Lead   \n",
       "\n",
       "  discourse_type_num                                   predictionstring  \\\n",
       "0             Lead 1  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 1...   \n",
       "\n",
       "   previous_discourse_flag  predictionstring_len  \\\n",
       "0                        0                    44   \n",
       "\n",
       "                                               label  \n",
       "0  B-Lead I-Lead I-Lead I-Lead I-Lead I-Lead I-Le...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vectorize labelizer func and apply to our df \n",
    "labelizer_vect = np.vectorize(labelizer)\n",
    "df['label']=labelizer_vect(df['discourse_type'],df['predictionstring_len'],df['previous_discourse_flag'])\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58b4fc6d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T14:43:58.352503Z",
     "start_time": "2022-02-08T14:43:56.368060Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>discourse_id</th>\n",
       "      <th>discourse_start</th>\n",
       "      <th>discourse_end</th>\n",
       "      <th>discourse_text</th>\n",
       "      <th>discourse_type</th>\n",
       "      <th>discourse_type_num</th>\n",
       "      <th>predictionstring</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>1.622628e+12</td>\n",
       "      <td>8.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>Modern humans today are always on their phone....</td>\n",
       "      <td>Lead</td>\n",
       "      <td>Lead 1</td>\n",
       "      <td>1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>1.622628e+12</td>\n",
       "      <td>230.0</td>\n",
       "      <td>312.0</td>\n",
       "      <td>They are some really bad consequences when stu...</td>\n",
       "      <td>Position</td>\n",
       "      <td>Position 1</td>\n",
       "      <td>45 46 47 48 49 50 51 52 53 54 55 56 57 58 59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>1.622628e+12</td>\n",
       "      <td>313.0</td>\n",
       "      <td>401.0</td>\n",
       "      <td>Some certain areas in the United States ban ph...</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Evidence 1</td>\n",
       "      <td>60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id  discourse_id  discourse_start  discourse_end  \\\n",
       "0  423A1CA112E2  1.622628e+12              8.0          229.0   \n",
       "1  423A1CA112E2  1.622628e+12            230.0          312.0   \n",
       "2  423A1CA112E2  1.622628e+12            313.0          401.0   \n",
       "\n",
       "                                      discourse_text discourse_type  \\\n",
       "0  Modern humans today are always on their phone....           Lead   \n",
       "1  They are some really bad consequences when stu...       Position   \n",
       "2  Some certain areas in the United States ban ph...       Evidence   \n",
       "\n",
       "  discourse_type_num                                   predictionstring  \n",
       "0             Lead 1  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 1...  \n",
       "1         Position 1       45 46 47 48 49 50 51 52 53 54 55 56 57 58 59  \n",
       "2         Evidence 1    60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>predictionstring</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000D23A521A</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "      <td>[B-Position, I-Position, I-Position, I-Positio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00066EA9880D</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "      <td>[B-Lead, I-Lead, I-Lead, I-Lead, I-Lead, I-Lea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000E6DE9E817</td>\n",
       "      <td>[2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 14, 15, ...</td>\n",
       "      <td>[B-Position, I-Position, I-Position, I-Positio...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                   predictionstring  \\\n",
       "0  0000D23A521A  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...   \n",
       "1  00066EA9880D  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...   \n",
       "2  000E6DE9E817  [2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 14, 15, ...   \n",
       "\n",
       "                                               label  \n",
       "0  [B-Position, I-Position, I-Position, I-Positio...  \n",
       "1  [B-Lead, I-Lead, I-Lead, I-Lead, I-Lead, I-Lea...  \n",
       "2  [B-Position, I-Position, I-Position, I-Positio...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Groupby ID to get predictionstrings and labels as a unique string\n",
    "\n",
    "df_essays = df.groupby('id').agg({'predictionstring':' '.join,'label':' '.join})\n",
    "\n",
    "## Transform into lists\n",
    "\n",
    "df_essays['label'] = df_essays['label'].apply(lambda txt : txt.split())\n",
    "df_essays['predictionstring'] = df_essays['predictionstring'].apply(lambda txt : txt.split())\n",
    "\n",
    "#remove utilities columns created in the original df\n",
    "df.drop(['previous_discourse_flag','predictionstring_len','label'],axis=1,inplace=True)\n",
    "\n",
    "#reset index\n",
    "df_essays.reset_index(inplace=True)\n",
    "\n",
    "display(df.head(3),df_essays.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592ef22e",
   "metadata": {},
   "source": [
    "## Create dataframe for the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a898c54",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T14:43:58.404985Z",
     "start_time": "2022-02-08T14:43:58.397459Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DF920E0A7337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0FB0700DAF44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D46BCB48440A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18409261F5C2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>D72CB1C11673</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id\n",
       "0  DF920E0A7337\n",
       "1  0FB0700DAF44\n",
       "2  D46BCB48440A\n",
       "3  18409261F5C2\n",
       "4  D72CB1C11673"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = [t.split('.')[0] for t in os.listdir(os.path.join(PATH_RAW_DATA,'test'))]\n",
    "df_test = pd.DataFrame(ids,columns=['id'])\n",
    "df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4a7ee9",
   "metadata": {},
   "source": [
    "## Retrieve full text properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbed858f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T14:44:00.440219Z",
     "start_time": "2022-02-08T14:44:00.435732Z"
    }
   },
   "outputs": [],
   "source": [
    "#Function\n",
    "\n",
    "def get_essay(id_,mode='train'):\n",
    "    \"\"\"Function to get the full text of an essay from the .txt file.\n",
    "\n",
    "    Args:\n",
    "        id_ (str): id of the essay\n",
    "        mode (str, optional): determines whether to access *train* or *test* texts. \\\n",
    "        Defaults to 'train'.\n",
    "\n",
    "    Returns:\n",
    "        str: Returns the full text of the id\n",
    "    \"\"\"\n",
    "    with open(os.path.join(PATH_RAW_DATA,mode,f'{id_}.txt'),'r') as file:\n",
    "        txt = file.read()\n",
    "        return txt.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47b29dc",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d0c7d8a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T14:44:00.816736Z",
     "start_time": "2022-02-08T14:44:00.807934Z"
    }
   },
   "outputs": [],
   "source": [
    "#Function\n",
    "\n",
    "def tokenize_labelize(essay,tokenizer,predictionstring=None,labels=None,max_len=SEQ_LEN):\n",
    "    \"\"\"Tokenize an essay and match each token with the corresponding label.\n",
    "\n",
    "    Args:\n",
    "        essay (str): Text to tokenize\n",
    "        tokenizer (tokenizer): Tokenizer from HF.\n",
    "        predictionstring (pandas.Series | numpy.array, optional): As a unique string, list of index position of words with a label. Must be provided with labels. Defaults to None.\n",
    "        labels (pandas.Series | numpy.array, optional): As a unique string, list of labels of each word. Must be provided with labels. Defaults to None.\n",
    "        max_len (int): Maximum sequence length for padding/truncating.\n",
    "        \n",
    "\n",
    "    Returns:\n",
    "        dict : Returns a dictionnary with input_ids,attention_mask and labels if passed.\n",
    "    \"\"\"\n",
    "    \n",
    "    tokens = tokenizer(essay,\n",
    "                       return_attention_mask = True,\n",
    "                       return_token_type_ids = False,\n",
    "                       padding = 'max_length',\n",
    "                       max_length = SEQ_LEN,\n",
    "                       truncation = True,\n",
    "                       return_tensors='np'\n",
    "                      )\n",
    "    \n",
    "    word_ids=tokens.word_ids()\n",
    "    \n",
    "    labels_mapping = {\n",
    "                      'B-Lead' : 0,\n",
    "                      'B-Position' : 1,\n",
    "                      'B-Evidence' : 2,\n",
    "                      'B-Claim' : 3,\n",
    "                      'B-Concluding_Statement' : 4,\n",
    "                      'B-Counterclaim' : 5,\n",
    "                      'B-Rebuttal' : 6,\n",
    "                      'I-Lead' : 7,\n",
    "                      'I-Position' : 8,\n",
    "                      'I-Evidence' : 9,\n",
    "                      'I-Claim' : 10,\n",
    "                      'I-Concluding_Statement' : 11,\n",
    "                      'I-Counterclaim' : 12,\n",
    "                      'I-Rebuttal': 13\n",
    "                        }\n",
    "    \n",
    "    if labels:\n",
    "        match = {p:labels_mapping[l] for p,l in zip(predictionstring,labels)}\n",
    "        labels_matched = [15 if (w==None or w==word_ids[i-1]) \\\n",
    "                            else match.get(str(w),14) \\\n",
    "                            for i,w in enumerate(word_ids)]\n",
    "                            \n",
    "        \n",
    "        return {\n",
    "        'input_ids' : tokens['input_ids'][0],\n",
    "        'attention_mask' : tokens['attention_mask'][0],\n",
    "        'labels': np.array(labels_matched), \n",
    "        'predictionstring':np.array(word_ids)\n",
    "        }\n",
    "    \n",
    "    return {\n",
    "        'input_ids' : tokens['input_ids'][0],\n",
    "        'attention_mask' : tokens['attention_mask'][0],\n",
    "        'predictionstring':np.array(word_ids)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d43342b",
   "metadata": {},
   "source": [
    "## Create preprocessed dataset\n",
    "\n",
    "Working per batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd001f31",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T14:44:06.881237Z",
     "start_time": "2022-02-08T14:44:02.144356Z"
    }
   },
   "outputs": [],
   "source": [
    "## Instantiate tokenizer from HF\n",
    "tokenizer = AutoTokenizer.from_pretrained('allenai/longformer-base-4096')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f321835",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T14:44:06.947593Z",
     "start_time": "2022-02-08T14:44:06.943437Z"
    }
   },
   "outputs": [],
   "source": [
    "## vectorize the function tokenizer above\n",
    "tokenize_labelize_vect = np.vectorize(tokenize_labelize,excluded=['SEQ_LEN'],otypes=['object'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05a661ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T14:44:49.853401Z",
     "start_time": "2022-02-08T14:44:07.012677Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab59010535b54b17b02f0614d1f083fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing...:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create CSV file with tokens (input_ids, attention_mask, predictionstring, labels)\n",
    "# also stored in an array tokens\n",
    "\n",
    "batch_size = 50\n",
    "nbatch = int(len(df_essays)/batch_size)+1\n",
    "\n",
    "fieldnames = ['id','predictionstring','label','essays']\n",
    "tokens = np.array([])\n",
    "\n",
    "with open(PATH_RAW_DATA+NAME_OUTPUT_FILE,'w') as file :\n",
    "    writer = csv.DictWriter(file,fieldnames = fieldnames)\n",
    "    writer.writeheader()\n",
    "\n",
    "for i in tqdm(range(nbatch+1),desc='Processing...'):\n",
    "    df_ = df_essays.loc[i*batch_size:(i+1)*batch_size-1].copy()\n",
    "    df_['essays'] = df_['id'].apply(get_essay)\n",
    "    tokens = np.append(tokens,tokenize_labelize_vect(df_.essays,tokenizer,\n",
    "                                           df_.predictionstring, df_.label ,max_len=SEQ_LEN))\n",
    "    df_.to_csv(PATH_RAW_DATA+NAME_OUTPUT_FILE,mode='a',header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bfeff648",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T14:45:10.862521Z",
     "start_time": "2022-02-08T14:45:09.561120Z"
    }
   },
   "outputs": [],
   "source": [
    "## sanity check\n",
    "result = pd.read_csv(PATH_RAW_DATA+NAME_OUTPUT_FILE)\n",
    "assert(tokens.shape[0]==result.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4455a2dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T14:45:11.829025Z",
     "start_time": "2022-02-08T14:45:11.804533Z"
    }
   },
   "outputs": [],
   "source": [
    "##create tokens_test array\n",
    "\n",
    "df_test['essays'] = df_test['id'].apply(get_essay,mode='test')\n",
    "tokens_test = tokenize_labelize_vect(df_test.essays,tokenizer,max_len=SEQ_LEN)\n",
    "\n",
    "## saving as csv\n",
    "df_test.to_csv(PATH_RAW_DATA+f'preprocessed_inf_v{VERSION}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170b3d19",
   "metadata": {},
   "source": [
    "## Creating dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3de728d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T14:45:13.068754Z",
     "start_time": "2022-02-08T14:45:13.061004Z"
    }
   },
   "outputs": [],
   "source": [
    "def dataset_creator(tokens):\n",
    "    \"\"\"\n",
    "    Creates a dictionnary with tokens attributes as a numpy array.\n",
    "\n",
    "    Args:\n",
    "        tokens (list): list of dictionnaries, outputs from tokeniner\n",
    "\n",
    "    Returns:\n",
    "        dict: dict with list of size BATCH_SIZE of inputs_id, attention mask, predictionstring and labels if provided.\n",
    "    \"\"\"\n",
    "    \n",
    "    keys = tokens[0].keys()\n",
    "\n",
    "    inputs = {\n",
    "        'input_ids':[],\n",
    "        'attention_mask':[]\n",
    "        }\n",
    "    predictionstring = []\n",
    "    labels = []\n",
    "       \n",
    "    for t in tqdm(tokens,desc='Aggregating dataset'):\n",
    "        inputs['input_ids'].append(t['input_ids'])\n",
    "        inputs['attention_mask'].append(t['attention_mask'])\n",
    "        predictionstring.append(t['predictionstring'])\n",
    "        if 'labels' in keys:\n",
    "                labels.append(t['labels'])\n",
    "\n",
    "        \n",
    "    inputs['input_ids'] = np.array(inputs['input_ids'])\n",
    "    inputs['attention_mask'] = np.array(inputs['attention_mask'])\n",
    "    predictionstring = np.array(predictionstring)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    if 'labels' in tokens[0].keys():\n",
    "        \n",
    "        #OHE labels\n",
    "        labels_ohe = np.zeros((len(labels),SEQ_LEN,16))\n",
    "        \n",
    "        dim1 = np.arange(len(labels))\n",
    "        dim2 = np.arange(SEQ_LEN)\n",
    "        \n",
    "        labels_ohe[dim1[:,None,None],dim2[None,:,None],labels[:,:,None]] = 1\n",
    "        \n",
    "        return inputs, labels_ohe, predictionstring\n",
    "    \n",
    "    return inputs, predictionstring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9465810d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T14:45:16.332931Z",
     "start_time": "2022-02-08T14:45:14.405516Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75eb0c2306cc4c9fadfadc996b4dd527",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Aggregating dataset:   0%|          | 0/15594 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#training dataset\n",
    "if 'labels' in tokens[0].keys():\n",
    "    inputs,labels,predictionstrings = dataset_creator(tokens)\n",
    "else:\n",
    "    inputs,predictionstrings = dataset_creator(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a6d73d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T14:45:16.428433Z",
     "start_time": "2022-02-08T14:45:16.403039Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31d4ddcc0d054a0f8e307e1078eee85f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Aggregating dataset:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## creating test dataset\n",
    "inputs_test,ps_test = dataset_creator(tokens_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7f188d",
   "metadata": {},
   "source": [
    "# Save datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4d6e94b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T14:45:18.071428Z",
     "start_time": "2022-02-08T14:45:18.065803Z"
    }
   },
   "outputs": [],
   "source": [
    "## Store all objects in a single dictionnary for training\n",
    "\n",
    "if 'labels' in tokens[0].keys():\n",
    "    dataset = {\n",
    "        'inputs':inputs,\n",
    "        'labels':labels,\n",
    "        'predictionstrings':predictionstrings\n",
    "    }\n",
    "else:\n",
    "    dataset = {\n",
    "        'inputs':inputs,\n",
    "        'predictionstrings':predictionstrings\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d09bf01c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T14:45:19.483253Z",
     "start_time": "2022-02-08T14:45:19.480023Z"
    }
   },
   "outputs": [],
   "source": [
    "## store test objects stored in a dict\n",
    "\n",
    "dataset_test = {\n",
    "        'inputs':inputs_test,\n",
    "        'predictionstrings':ps_test\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "321206c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T14:45:27.162178Z",
     "start_time": "2022-02-08T14:45:20.510478Z"
    }
   },
   "outputs": [],
   "source": [
    "## dump dataset dictionnary with as a pickle file\n",
    "\n",
    "with open(f'../raw_data/dataset_v{VERSION}.pickle','wb') as file : \n",
    "    pickle.dump(dataset,file)\n",
    "    \n",
    "with open(f'../raw_data/dataset_test_v{VERSION}.pickle','wb') as file : \n",
    "    pickle.dump(dataset_test,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7fefe1cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T13:23:34.140576Z",
     "start_time": "2022-02-08T13:23:34.137608Z"
    }
   },
   "outputs": [],
   "source": [
    "### the end ###"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
