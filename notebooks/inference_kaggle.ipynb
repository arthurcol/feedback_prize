{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8cc55ac",
   "metadata": {},
   "source": [
    "# Imports and Variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2046fa6e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T13:16:56.018595Z",
     "start_time": "2022-02-09T13:16:56.012730Z"
    }
   },
   "outputs": [],
   "source": [
    "#variables \n",
    "\n",
    "SEQ_LEN = 1024 \n",
    "\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "\n",
    "#PATHS \n",
    "\n",
    "LOAD_BACKBONE_FROM = '../input/backbone/'\n",
    "LOAD_MODEL_WEIGHTS_FROM = '../input/mymodel/mymodel'\n",
    "LOAD_TXT_FROM = '../input/feedback-prize-2021/test/'\n",
    "\n",
    "#GPU and info message for tf\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='0' \n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76745598",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T13:16:14.992257Z",
     "start_time": "2022-02-09T13:16:14.981509Z"
    }
   },
   "outputs": [],
   "source": [
    "#utils \n",
    "import os \n",
    "import pickle \n",
    "\n",
    "# canonicals \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "#deep\n",
    "import tensorflow as tf\n",
    "from transformers import TFAutoModel, AutoConfig, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78964cc6",
   "metadata": {},
   "source": [
    "# Loading and preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3228cb9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T13:18:16.816910Z",
     "start_time": "2022-02-09T13:18:16.811361Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DF920E0A7337',\n",
       " '0FB0700DAF44',\n",
       " 'D46BCB48440A',\n",
       " '18409261F5C2',\n",
       " 'D72CB1C11673']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_ids = [f.split('.')[0] for f in os.listdir(LOAD_TXT_FROM)]\n",
    "df_test = pd.DataFrame(txt_ids,columns=['id'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96ee7423",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T13:23:48.855360Z",
     "start_time": "2022-02-09T13:23:48.851160Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_essay(id_):\n",
    "    \"\"\"Function to get the full text of an essay from the .txt file.\n",
    "\n",
    "    Args:\n",
    "        id_ (str): id of the essay\n",
    "        mode (str, optional): determines whether to access *train* or *test* texts. \\\n",
    "        Defaults to 'train'.\n",
    "\n",
    "    Returns:\n",
    "        str: Returns the full text of the id\n",
    "    \"\"\"\n",
    "    with open(os.path.join(LOAD_TXT_FROM,f'{id_}.txt'),'r') as file:\n",
    "        txt = file.read()\n",
    "        return txt.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b694a64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store text in df\n",
    "df_test['essays'] = df_test['id'].apply(get_essay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2929804",
   "metadata": {},
   "source": [
    "# Tokenize texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318a841c",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93d40dfb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T13:22:17.698739Z",
     "start_time": "2022-02-09T13:22:17.669997Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize_labelize(essay,tokenizer,predictionstring=None,labels=None,max_len=SEQ_LEN):\n",
    "    \"\"\"Tokenize an essay and match each token with the corresponding label.\n",
    "\n",
    "    Args:\n",
    "        essay (str): Text to tokenize\n",
    "        tokenizer (tokenizer): Tokenizer from HF.\n",
    "        predictionstring (pandas.Series | numpy.array, optional): As a unique string, list of index position of words with a label. Must be provided with labels. Defaults to None.\n",
    "        labels (pandas.Series | numpy.array, optional): As a unique string, list of labels of each word. Must be provided with labels. Defaults to None.\n",
    "        max_len (int): Maximum sequence length for padding/truncating.\n",
    "        \n",
    "\n",
    "    Returns:\n",
    "        dict : Returns a dictionnary with input_ids,attention_mask and labels if passed.\n",
    "    \"\"\"\n",
    "    \n",
    "    tokens = tokenizer(essay,\n",
    "                       return_attention_mask = True,\n",
    "                       return_token_type_ids = False,\n",
    "                       padding = 'max_length',\n",
    "                       max_length = SEQ_LEN,\n",
    "                       truncation = True,\n",
    "                       return_tensors='np'\n",
    "                      )\n",
    "    \n",
    "    word_ids=tokens.word_ids()\n",
    "    \n",
    "    labels_mapping = {\n",
    "                      'B-Lead' : 0,\n",
    "                      'B-Position' : 1,\n",
    "                      'B-Evidence' : 2,\n",
    "                      'B-Claim' : 3,\n",
    "                      'B-Concluding_Statement' : 4,\n",
    "                      'B-Counterclaim' : 5,\n",
    "                      'B-Rebuttal' : 6,\n",
    "                      'I-Lead' : 7,\n",
    "                      'I-Position' : 8,\n",
    "                      'I-Evidence' : 9,\n",
    "                      'I-Claim' : 10,\n",
    "                      'I-Concluding_Statement' : 11,\n",
    "                      'I-Counterclaim' : 12,\n",
    "                      'I-Rebuttal': 13\n",
    "                        }\n",
    "    \n",
    "    if labels:\n",
    "        match = {p:labels_mapping[l] for p,l in zip(predictionstring,labels)}\n",
    "        labels_matched = [15 if (w==None or w==word_ids[i-1]) \\\n",
    "                            else match.get(str(w),14) \\\n",
    "                            for i,w in enumerate(word_ids)]\n",
    "                            \n",
    "        \n",
    "        return {\n",
    "        'input_ids' : tokens['input_ids'][0],\n",
    "        'attention_mask' : tokens['attention_mask'][0],\n",
    "        'labels': np.array(labels_matched), \n",
    "        'predictionstring':np.array(word_ids)\n",
    "        }\n",
    "    \n",
    "    return {\n",
    "        'input_ids' : tokens['input_ids'][0],\n",
    "        'attention_mask' : tokens['attention_mask'][0],\n",
    "        'predictionstring':np.array(word_ids)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83b7f9bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T13:22:43.130645Z",
     "start_time": "2022-02-09T13:22:43.113541Z"
    }
   },
   "outputs": [],
   "source": [
    "## vectorize the function tokenizer above\n",
    "tokenize_labelize_vect = np.vectorize(tokenize_labelize,excluded=['SEQ_LEN'],otypes=['object'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3317b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(LOAD_BACKBONE_FROM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54cc89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create tokens\n",
    "tokens_test = tokenize_labelize_vect(df_test.essays,tokenizer,max_len=SEQ_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd20ce48",
   "metadata": {},
   "source": [
    "## Dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6f353568",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T13:29:39.724058Z",
     "start_time": "2022-02-09T13:29:39.716081Z"
    }
   },
   "outputs": [],
   "source": [
    "def dataset_creator(tokens):\n",
    "    \"\"\"\n",
    "    Creates a dictionnary with tokens attributes as a numpy array.\n",
    "\n",
    "    Args:\n",
    "        tokens (list): list of dictionnaries, outputs from tokeniner\n",
    "\n",
    "    Returns:\n",
    "        dict: dict with list of size BATCH_SIZE of inputs_id, attention mask, predictionstring and labels if provided.\n",
    "    \"\"\"\n",
    "    \n",
    "    keys = tokens[0].keys()\n",
    "\n",
    "    inputs = {\n",
    "        'input_ids':[],\n",
    "        'attention_mask':[]\n",
    "        }\n",
    "    predictionstring = []\n",
    "    labels = []\n",
    "       \n",
    "    for t in tokens:\n",
    "        inputs['input_ids'].append(t['input_ids'])\n",
    "        inputs['attention_mask'].append(t['attention_mask'])\n",
    "        predictionstring.append(t['predictionstring'])\n",
    "        if 'labels' in keys:\n",
    "                labels.append(t['labels'])\n",
    "\n",
    "        \n",
    "    inputs['input_ids'] = np.array(inputs['input_ids'])\n",
    "    inputs['attention_mask'] = np.array(inputs['attention_mask'])\n",
    "    predictionstring = np.array(predictionstring)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    if 'labels' in tokens[0].keys():\n",
    "        \n",
    "        #OHE labels\n",
    "        labels_ohe = np.zeros((len(labels),SEQ_LEN,16))\n",
    "        \n",
    "        dim1 = np.arange(len(labels))\n",
    "        dim2 = np.arange(SEQ_LEN)\n",
    "        \n",
    "        labels_ohe[dim1[:,None,None],dim2[None,:,None],labels[:,:,None]] = 1\n",
    "        \n",
    "        return inputs, labels_ohe, predictionstring\n",
    "    \n",
    "    return inputs, predictionstring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880516fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "## creating test dataset\n",
    "X_test,ps_test = dataset_creator(tokens_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986af77e",
   "metadata": {},
   "source": [
    "# Model prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eef8ea3",
   "metadata": {},
   "source": [
    "## Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d50a684f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T13:37:01.648522Z",
     "start_time": "2022-02-09T13:37:01.158808Z"
    }
   },
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "404 Client Error: Not Found for url: https://huggingface.co/api/input/backbone/config.json",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/zd/9ts3jb2s5472mty2sgdddp3m0000gn/T/ipykernel_20569/3054337654.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## Instantiate model Longformer to be used as backbone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLOAD_BACKBONE_FROM\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'config.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mbackbone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTFAutoModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLOAD_BACKBONE_FROM\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'tf_model.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.12/envs/feedback-prize/lib/python3.8/site-packages/transformers/models/auto/configuration_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    580\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"name_or_path\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0mtrust_remote_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"trust_remote_code\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 582\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    583\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"auto_map\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"AutoConfig\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"auto_map\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.12/envs/feedback-prize/lib/python3.8/site-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0mconfig_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m             configuration_file = get_configuration_file(\n\u001b[0m\u001b[1;32m    553\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m                 \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.12/envs/feedback-prize/lib/python3.8/site-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_configuration_file\u001b[0;34m(path_or_repo, revision, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m    841\u001b[0m     \"\"\"\n\u001b[1;32m    842\u001b[0m     \u001b[0;31m# Inspect all files from the repo/folder.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 843\u001b[0;31m     all_files = get_list_of_files(\n\u001b[0m\u001b[1;32m    844\u001b[0m         \u001b[0mpath_or_repo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_files_only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m     )\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.12/envs/feedback-prize/lib/python3.8/site-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mget_list_of_files\u001b[0;34m(path_or_repo, revision, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m   2101\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2102\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2103\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mlist_repo_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_repo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.12/envs/feedback-prize/lib/python3.8/site-packages/huggingface_hub/hf_api.py\u001b[0m in \u001b[0;36mlist_repo_files\u001b[0;34m(self, repo_id, revision, repo_type, token, timeout)\u001b[0m\n\u001b[1;32m    600\u001b[0m         \"\"\"\n\u001b[1;32m    601\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrepo_type\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m             info = self.model_info(\n\u001b[0m\u001b[1;32m    603\u001b[0m                 \u001b[0mrepo_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m             )\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.12/envs/feedback-prize/lib/python3.8/site-packages/huggingface_hub/hf_api.py\u001b[0m in \u001b[0;36mmodel_info\u001b[0;34m(self, repo_id, revision, token, timeout)\u001b[0m\n\u001b[1;32m    584\u001b[0m         )\n\u001b[1;32m    585\u001b[0m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m         \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m         \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mModelInfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.12/envs/feedback-prize/lib/python3.8/site-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 953\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    954\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://huggingface.co/api/input/backbone/config.json"
     ]
    }
   ],
   "source": [
    "## Instantiate model Longformer to be used as backbone\n",
    "config = AutoConfig.from_pretrained(os.path.join(LOAD_BACKBONE_FROM,'config.json'))\n",
    "backbone = TFAutoModel.from_pretrained(os.path.join(LOAD_BACKBONE_FROM,'tf_model.h5'),config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7970acf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model():\n",
    "    input_ids = tf.keras.layers.Input(shape=(SEQ_LEN,),dtype='int32')\n",
    "    attention_mask = tf.keras.layers.Input(shape=(SEQ_LEN,),dtype='int32')\n",
    "    \n",
    "    x = backbone({'input_ids':input_ids,\n",
    "                 'attention_mask':attention_mask})[0]\n",
    "\n",
    "    backbone.trainable = False\n",
    "\n",
    "\n",
    "    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units = 512,\n",
    "                                                           activation = 'tanh',\n",
    "                                                           dropout=.2,\n",
    "                                                           return_sequences=True))(x)\n",
    "    x_res = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units = 512,\n",
    "                                                           activation = 'tanh',\n",
    "                                                           dropout=.2,\n",
    "                                                           return_sequences=True))(x)\n",
    "    \n",
    "\n",
    "    x = tf.keras.layers.add([x,x_res])\n",
    "    output = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(16,activation = 'softmax'))(x)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs={'input_ids':input_ids,\n",
    "                                          'attention_mask':attention_mask},outputs=output)\n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06491b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## instantiate the model, plot the graph\n",
    "model = init_model()\n",
    "\n",
    "tf.keras.utils.plot_model(model,show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fa625b",
   "metadata": {},
   "source": [
    "## Model compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e91b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating homemade metric\n",
    "\n",
    "def accuracy_masked_func(y_true,y_pred):\n",
    "    y_pred = tf.cast(tf.argmax(y_pred,axis=-1),'int32')\n",
    "    y_true = tf.cast(y_true,'int32')\n",
    "    y_true = tf.cast(tf.argmax(y_true,axis=-1),'int32') #for y_pred and y_true to match\n",
    "    mask = tf.cast(y_true != 15,'int32') #create a mask\n",
    "    matches = tf.cast(tf.equal(y_true,y_pred),'int32')*mask #calculate the matches ignoring the masking\n",
    "    accuracy = tf.math.reduce_sum(matches,axis=-1)/tf.maximum(tf.math.reduce_sum(mask,axis=-1),1)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0de11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss and metrics \n",
    "loss = tf.keras.losses.CategoricalCrossentropy(name='categorical_crossentropy')\n",
    "cat_accuracy = tf.keras.metrics.CategoricalAccuracy()\n",
    "masked_accuracy = tf.keras.metrics.MeanMetricWrapper(fn=accuracy_masked_func)\n",
    "\n",
    "# RMSProp optimizer with clip value and small lr to avoid exploiding gradient \n",
    "opt = tf.keras.optimizers.RMSprop(clipvalue=.5,learning_rate=0.0001)\n",
    "\n",
    "#compile\n",
    "model.compile(optimizer=opt,loss=loss,metrics=[cat_accuracy,masked_accuracy])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ad84bc",
   "metadata": {},
   "source": [
    "## Load model pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd53d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(os.path.join(LOAD_MODEL_WEIGHTS_FROM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed8bc47",
   "metadata": {},
   "source": [
    "# Get predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e53b9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test,batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b3488c",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_mapping = {'B-Lead' : 0,\n",
    "                  'B-Position' : 1,\n",
    "                  'B-Evidence' : 2,\n",
    "                  'B-Claim' : 3,\n",
    "                  'B-Concluding_Statement' : 4,\n",
    "                  'B-Counterclaim' : 5,\n",
    "                  'B-Rebuttal' : 6,\n",
    "                  'I-Lead' : 7,\n",
    "                  'I-Position' : 8,\n",
    "                  'I-Evidence' : 9,\n",
    "                  'I-Claim' : 10,\n",
    "                  'I-Concluding_Statement' : 11,\n",
    "                  'I-Counterclaim' : 12,\n",
    "                  'I-Rebuttal': 13,\n",
    "                 'O':14,\n",
    "                 'PAD':15}\n",
    "\n",
    "reversed_mapping = {v:(k[2:] if v<14 else k) for k,v in labels_mapping.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0920824",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preds(y_pred,ps):\n",
    "    \"\"\"\n",
    "    Generate readable predictions from the output of the model.\n",
    "\n",
    "    Args:\n",
    "        y_pred (ndarray): output of the model\n",
    "        ps (ndarray): predictionstring referring to the token predicted\n",
    "\n",
    "    Returns:\n",
    "        DataFrame : DataFrame with class and predictionstrings\n",
    "    \"\"\"\n",
    "    \n",
    "    labels = []\n",
    "    predictionstrings = []\n",
    "    counts = []\n",
    "    \n",
    "    counter=dict()\n",
    "    \n",
    "    for tok,pos in zip(y_pred,ps):\n",
    "        \n",
    "        if tok <= 13:\n",
    "            lab = reversed_mapping[tok]\n",
    "            labels.append(lab)\n",
    "            predictionstrings.append(pos)\n",
    "            if len(labels)<2:\n",
    "                counts.append(str(1))\n",
    "                counter.setdefault(lab,1)\n",
    "                continue\n",
    "            if lab == labels[-2]:\n",
    "                counts.append(str(counter[lab]))\n",
    "            else: \n",
    "                try:\n",
    "                    counter[lab]+=1\n",
    "                except KeyError:\n",
    "                    counter.setdefault(lab,1)\n",
    "                counts.append(str(counter[lab]))\n",
    "    \n",
    "    preds = pd.DataFrame([labels,counts,predictionstrings],index=['class','count','predictionstring']).T\n",
    "    preds['class'] += ' ' + preds['count'].astype(str)\n",
    "    preds = preds.groupby('class',sort=False).agg({'predictionstring':list}).reset_index()\n",
    "    preds['class']=preds['class'].apply(lambda txt : txt.split()[0])\n",
    "    preds['predictionstring']=preds['predictionstring'].apply(lambda l_ : [str(l) for l in l_])\n",
    "    preds['predictionstring']=preds['predictionstring'].apply(lambda l_ : ' '.join(l_))\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df301f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.argmax(y_pred,axis=-1)\n",
    "preds_df = pd.DataFrame()\n",
    "\n",
    "for i,idx in enumerate(df_test.index): \n",
    "    \n",
    "    pred_ = get_preds(preds[i],ps_test[i])\n",
    "    \n",
    "    pred_['id']=df_test.iloc[idx]['id']\n",
    "    \n",
    "    preds_df = preds_df.append(pred_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5da4c4",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f24a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = preds_df[['id','class','predictionstring']]\n",
    "sub.reset_index(inplace=True,drop=True)\n",
    "sub.to_csv('submission.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
